{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnkevinoregan/Learning-Relations-with-Deep-Learning/blob/main/Bright_dark_comparison_task_DONT_CHANGE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Brightness comparison on 1D retina: generalization untrained brightnesses\n",
        "15 Mar 2023 -- work in prog\n",
        "\n",
        "---\n",
        "\n",
        "This program is not yet finished. \n",
        "\n",
        "The idea: There are two pixels lit up on the Left and Right of the retina. The NN is to respond 1 when the left one is brighter than the Right one. I train on a collection of Brightnesses with equal numbers of 0 and 1 responses. I then test on these plus the same combinations at retinal positions more widely separated by distance DeltaPos in range(Deltas) from the training locations.  \n",
        "\n",
        "problems: \n",
        "\n",
        "the test set contains the case when the two brightnesses are equal. I should get rid of that.\n",
        "\n",
        "In plotting the results: at the moment I've just plotted the prediction for different values of brightness of the left-hand pixel. But I should do this separately for different retinal positions, and for different values of the correct answer (ie whether it was brighter or dimmer than the right-hand pixel). \n",
        "\n",
        "There's something funny about the results in the normal NN case!!??"
      ],
      "metadata": {
        "id": "dpNBOy9U6vG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "RetSize=18\n",
        "Left=int(RetSize/4)\n",
        "Right=3*Left\n",
        "TrainRepeats=3\n",
        "Deltas=3 #number of deviations from base retinal positions used in test\n",
        "\n",
        "W=3 #half the number of brightnesses in the test set\n",
        "W2=2*W\n",
        "\n",
        "Brightnesses=np.arange(-W2,W2+1)*16+125-8\n",
        "#print(Brightnesses)\n",
        "LBrightnesses=len(Brightnesses)\n",
        "#print('length',L)\n",
        "X_test=np.zeros((LBrightnesses*(1+Deltas),RetSize),int)\n",
        "y_test=np.zeros(LBrightnesses*(1+Deltas),int)\n",
        "res_test=np.zeros(LBrightnesses*(1+Deltas),float)\n",
        "\n",
        "X_train=np.zeros((2*TrainRepeats,RetSize),int)\n",
        "y_train=np.zeros(2*TrainRepeats,int)\n",
        "#print(X_train)\n",
        "\n",
        "#there's a problem in this loop since it \n",
        "trial=0\n",
        "for DeltaPos in range(Deltas+1):\n",
        "  for trial in range(LBrightnesses):\n",
        "    T=trial+DeltaPos*LBrightnesses\n",
        "    #print(T,Left-DeltaPos)\n",
        "    X_test[T,Left-DeltaPos]=Brightnesses[trial]\n",
        "    k=LBrightnesses-trial-1\n",
        "    X_test[T,Right+DeltaPos]=Brightnesses[k]\n",
        "    if X_test[T,Left-DeltaPos]<X_test[T,Right+DeltaPos]:\n",
        "      y_test[T]=1\n",
        "print('test stimuli')\n",
        "print(X_test)\n",
        "print('test labels')\n",
        "print(y_test)\n",
        "\n",
        "trial=0\n",
        "while trial <= TrainRepeats+1:\n",
        "  X_train[trial,Left]=Brightnesses[W]\n",
        "  X_train[trial,Right]=Brightnesses[LBrightnesses-W-1]\n",
        "  if X_train[trial,Left]<X_train[trial,Right]:\n",
        "    y_train[trial]=1\n",
        "  trial += 1\n",
        "  X_train[trial,Left]=Brightnesses[LBrightnesses-W-1]\n",
        "  X_train[trial,Right]=Brightnesses[W]\n",
        "  if X_train[trial,Left]<X_train[trial,Right]:\n",
        "    y_train[trial]=1\n",
        "  trial +=1\n",
        "\n",
        "print('training stimuli')\n",
        "print(X_train)\n",
        "print('training labels')\n",
        "print(y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiMlaDpwEAdp",
        "outputId": "7da4a826-2cf6-42e0-86d6-80a237880674"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test stimuli\n",
            "[[  0   0   0   0  21   0   0   0   0   0   0   0 213   0   0   0   0   0]\n",
            " [  0   0   0   0  37   0   0   0   0   0   0   0 197   0   0   0   0   0]\n",
            " [  0   0   0   0  53   0   0   0   0   0   0   0 181   0   0   0   0   0]\n",
            " [  0   0   0   0  69   0   0   0   0   0   0   0 165   0   0   0   0   0]\n",
            " [  0   0   0   0  85   0   0   0   0   0   0   0 149   0   0   0   0   0]\n",
            " [  0   0   0   0 101   0   0   0   0   0   0   0 133   0   0   0   0   0]\n",
            " [  0   0   0   0 117   0   0   0   0   0   0   0 117   0   0   0   0   0]\n",
            " [  0   0   0   0 133   0   0   0   0   0   0   0 101   0   0   0   0   0]\n",
            " [  0   0   0   0 149   0   0   0   0   0   0   0  85   0   0   0   0   0]\n",
            " [  0   0   0   0 165   0   0   0   0   0   0   0  69   0   0   0   0   0]\n",
            " [  0   0   0   0 181   0   0   0   0   0   0   0  53   0   0   0   0   0]\n",
            " [  0   0   0   0 197   0   0   0   0   0   0   0  37   0   0   0   0   0]\n",
            " [  0   0   0   0 213   0   0   0   0   0   0   0  21   0   0   0   0   0]\n",
            " [  0   0   0  21   0   0   0   0   0   0   0   0   0 213   0   0   0   0]\n",
            " [  0   0   0  37   0   0   0   0   0   0   0   0   0 197   0   0   0   0]\n",
            " [  0   0   0  53   0   0   0   0   0   0   0   0   0 181   0   0   0   0]\n",
            " [  0   0   0  69   0   0   0   0   0   0   0   0   0 165   0   0   0   0]\n",
            " [  0   0   0  85   0   0   0   0   0   0   0   0   0 149   0   0   0   0]\n",
            " [  0   0   0 101   0   0   0   0   0   0   0   0   0 133   0   0   0   0]\n",
            " [  0   0   0 117   0   0   0   0   0   0   0   0   0 117   0   0   0   0]\n",
            " [  0   0   0 133   0   0   0   0   0   0   0   0   0 101   0   0   0   0]\n",
            " [  0   0   0 149   0   0   0   0   0   0   0   0   0  85   0   0   0   0]\n",
            " [  0   0   0 165   0   0   0   0   0   0   0   0   0  69   0   0   0   0]\n",
            " [  0   0   0 181   0   0   0   0   0   0   0   0   0  53   0   0   0   0]\n",
            " [  0   0   0 197   0   0   0   0   0   0   0   0   0  37   0   0   0   0]\n",
            " [  0   0   0 213   0   0   0   0   0   0   0   0   0  21   0   0   0   0]\n",
            " [  0   0  21   0   0   0   0   0   0   0   0   0   0   0 213   0   0   0]\n",
            " [  0   0  37   0   0   0   0   0   0   0   0   0   0   0 197   0   0   0]\n",
            " [  0   0  53   0   0   0   0   0   0   0   0   0   0   0 181   0   0   0]\n",
            " [  0   0  69   0   0   0   0   0   0   0   0   0   0   0 165   0   0   0]\n",
            " [  0   0  85   0   0   0   0   0   0   0   0   0   0   0 149   0   0   0]\n",
            " [  0   0 101   0   0   0   0   0   0   0   0   0   0   0 133   0   0   0]\n",
            " [  0   0 117   0   0   0   0   0   0   0   0   0   0   0 117   0   0   0]\n",
            " [  0   0 133   0   0   0   0   0   0   0   0   0   0   0 101   0   0   0]\n",
            " [  0   0 149   0   0   0   0   0   0   0   0   0   0   0  85   0   0   0]\n",
            " [  0   0 165   0   0   0   0   0   0   0   0   0   0   0  69   0   0   0]\n",
            " [  0   0 181   0   0   0   0   0   0   0   0   0   0   0  53   0   0   0]\n",
            " [  0   0 197   0   0   0   0   0   0   0   0   0   0   0  37   0   0   0]\n",
            " [  0   0 213   0   0   0   0   0   0   0   0   0   0   0  21   0   0   0]\n",
            " [  0  21   0   0   0   0   0   0   0   0   0   0   0   0   0 213   0   0]\n",
            " [  0  37   0   0   0   0   0   0   0   0   0   0   0   0   0 197   0   0]\n",
            " [  0  53   0   0   0   0   0   0   0   0   0   0   0   0   0 181   0   0]\n",
            " [  0  69   0   0   0   0   0   0   0   0   0   0   0   0   0 165   0   0]\n",
            " [  0  85   0   0   0   0   0   0   0   0   0   0   0   0   0 149   0   0]\n",
            " [  0 101   0   0   0   0   0   0   0   0   0   0   0   0   0 133   0   0]\n",
            " [  0 117   0   0   0   0   0   0   0   0   0   0   0   0   0 117   0   0]\n",
            " [  0 133   0   0   0   0   0   0   0   0   0   0   0   0   0 101   0   0]\n",
            " [  0 149   0   0   0   0   0   0   0   0   0   0   0   0   0  85   0   0]\n",
            " [  0 165   0   0   0   0   0   0   0   0   0   0   0   0   0  69   0   0]\n",
            " [  0 181   0   0   0   0   0   0   0   0   0   0   0   0   0  53   0   0]\n",
            " [  0 197   0   0   0   0   0   0   0   0   0   0   0   0   0  37   0   0]\n",
            " [  0 213   0   0   0   0   0   0   0   0   0   0   0   0   0  21   0   0]]\n",
            "test labels\n",
            "[1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0\n",
            " 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0]\n",
            "training stimuli\n",
            "[[  0   0   0   0  69   0   0   0   0   0   0   0 165   0   0   0   0   0]\n",
            " [  0   0   0   0 165   0   0   0   0   0   0   0  69   0   0   0   0   0]\n",
            " [  0   0   0   0  69   0   0   0   0   0   0   0 165   0   0   0   0   0]\n",
            " [  0   0   0   0 165   0   0   0   0   0   0   0  69   0   0   0   0   0]\n",
            " [  0   0   0   0  69   0   0   0   0   0   0   0 165   0   0   0   0   0]\n",
            " [  0   0   0   0 165   0   0   0   0   0   0   0  69   0   0   0   0   0]]\n",
            "training labels\n",
            "[1 0 1 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Make training sets X_train and labels y_train"
      ],
      "metadata": {
        "id": "DEJCMNfpEqbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train with normal NN"
      ],
      "metadata": {
        "id": "TG23LV4t67V5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build, compile and train the neural network model\n",
        "#there are two hidden layers with 64 and 32 values\n",
        "#The ReLU activation function is used in deep learning models to introduce non-linearity into the network\n",
        "#the sigmoid activation function produces a probability value between 0 and 1\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "#from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(64, activation=\"relu\", input_dim=len(X_train[0])),\n",
        "    keras.layers.Dense(32, activation=\"relu\"),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "#model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "model.fit(X_train, y_train, epochs=6, batch_size=32)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ioqyu3kb7Tn0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "082695a2-cb92-4743-b63a-8ceb1ee7fd57"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "1/1 [==============================] - 1s 773ms/step - loss: 2.7365 - accuracy: 0.0000e+00\n",
            "Epoch 2/6\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.5709 - accuracy: 0.0000e+00\n",
            "Epoch 3/6\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7144 - accuracy: 0.5000\n",
            "Epoch 4/6\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2559 - accuracy: 1.0000\n",
            "Epoch 5/6\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0912 - accuracy: 1.0000\n",
            "Epoch 6/6\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0387 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd81105b100>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for trial in range(len(X_test)):\n",
        "  pred=model.predict(np.array([X_test[trial]]), verbose=0)\n",
        "  res_test[trial]=pred[0][0]\n",
        "  #print(np.around(pred[0][0],1),y_test[trial],X_test[trial])\n",
        "  print(np.around(res_test[trial],1),y_test[trial],X_test[trial])\n",
        "\n",
        "print(res_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WALjIVurdv1l",
        "outputId": "5a2ea85c-8db4-4d42-fd0d-aea0a9f36984"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9 1 [  0   0   0   0  21   0   0   0   0   0   0   0 213   0   0   0   0   0]\n",
            "0.9 1 [  0   0   0   0  37   0   0   0   0   0   0   0 197   0   0   0   0   0]\n",
            "1.0 1 [  0   0   0   0  53   0   0   0   0   0   0   0 181   0   0   0   0   0]\n",
            "1.0 1 [  0   0   0   0  69   0   0   0   0   0   0   0 165   0   0   0   0   0]\n",
            "1.0 1 [  0   0   0   0  85   0   0   0   0   0   0   0 149   0   0   0   0   0]\n",
            "0.8 1 [  0   0   0   0 101   0   0   0   0   0   0   0 133   0   0   0   0   0]\n",
            "0.3 0 [  0   0   0   0 117   0   0   0   0   0   0   0 117   0   0   0   0   0]\n",
            "0.0 0 [  0   0   0   0 133   0   0   0   0   0   0   0 101   0   0   0   0   0]\n",
            "0.0 0 [  0   0   0   0 149   0   0   0   0   0   0   0  85   0   0   0   0   0]\n",
            "0.0 0 [  0   0   0   0 165   0   0   0   0   0   0   0  69   0   0   0   0   0]\n",
            "0.1 0 [  0   0   0   0 181   0   0   0   0   0   0   0  53   0   0   0   0   0]\n",
            "0.2 0 [  0   0   0   0 197   0   0   0   0   0   0   0  37   0   0   0   0   0]\n",
            "0.5 0 [  0   0   0   0 213   0   0   0   0   0   0   0  21   0   0   0   0   0]\n",
            "0.1 1 [  0   0   0  21   0   0   0   0   0   0   0   0   0 213   0   0   0   0]\n",
            "0.0 1 [  0   0   0  37   0   0   0   0   0   0   0   0   0 197   0   0   0   0]\n",
            "0.0 1 [  0   0   0  53   0   0   0   0   0   0   0   0   0 181   0   0   0   0]\n",
            "0.0 1 [  0   0   0  69   0   0   0   0   0   0   0   0   0 165   0   0   0   0]\n",
            "0.0 1 [  0   0   0  85   0   0   0   0   0   0   0   0   0 149   0   0   0   0]\n",
            "0.0 1 [  0   0   0 101   0   0   0   0   0   0   0   0   0 133   0   0   0   0]\n",
            "0.1 0 [  0   0   0 117   0   0   0   0   0   0   0   0   0 117   0   0   0   0]\n",
            "0.6 0 [  0   0   0 133   0   0   0   0   0   0   0   0   0 101   0   0   0   0]\n",
            "0.8 0 [  0   0   0 149   0   0   0   0   0   0   0   0   0  85   0   0   0   0]\n",
            "0.7 0 [  0   0   0 165   0   0   0   0   0   0   0   0   0  69   0   0   0   0]\n",
            "0.7 0 [  0   0   0 181   0   0   0   0   0   0   0   0   0  53   0   0   0   0]\n",
            "0.9 0 [  0   0   0 197   0   0   0   0   0   0   0   0   0  37   0   0   0   0]\n",
            "1.0 0 [  0   0   0 213   0   0   0   0   0   0   0   0   0  21   0   0   0   0]\n",
            "0.0 1 [  0   0  21   0   0   0   0   0   0   0   0   0   0   0 213   0   0   0]\n",
            "0.0 1 [  0   0  37   0   0   0   0   0   0   0   0   0   0   0 197   0   0   0]\n",
            "0.0 1 [  0   0  53   0   0   0   0   0   0   0   0   0   0   0 181   0   0   0]\n",
            "0.0 1 [  0   0  69   0   0   0   0   0   0   0   0   0   0   0 165   0   0   0]\n",
            "0.0 1 [  0   0  85   0   0   0   0   0   0   0   0   0   0   0 149   0   0   0]\n",
            "0.0 1 [  0   0 101   0   0   0   0   0   0   0   0   0   0   0 133   0   0   0]\n",
            "0.0 0 [  0   0 117   0   0   0   0   0   0   0   0   0   0   0 117   0   0   0]\n",
            "0.0 0 [  0   0 133   0   0   0   0   0   0   0   0   0   0   0 101   0   0   0]\n",
            "0.0 0 [  0   0 149   0   0   0   0   0   0   0   0   0   0   0  85   0   0   0]\n",
            "0.0 0 [  0   0 165   0   0   0   0   0   0   0   0   0   0   0  69   0   0   0]\n",
            "0.0 0 [  0   0 181   0   0   0   0   0   0   0   0   0   0   0  53   0   0   0]\n",
            "0.0 0 [  0   0 197   0   0   0   0   0   0   0   0   0   0   0  37   0   0   0]\n",
            "0.0 0 [  0   0 213   0   0   0   0   0   0   0   0   0   0   0  21   0   0   0]\n",
            "1.0 1 [  0  21   0   0   0   0   0   0   0   0   0   0   0   0   0 213   0   0]\n",
            "1.0 1 [  0  37   0   0   0   0   0   0   0   0   0   0   0   0   0 197   0   0]\n",
            "1.0 1 [  0  53   0   0   0   0   0   0   0   0   0   0   0   0   0 181   0   0]\n",
            "1.0 1 [  0  69   0   0   0   0   0   0   0   0   0   0   0   0   0 165   0   0]\n",
            "1.0 1 [  0  85   0   0   0   0   0   0   0   0   0   0   0   0   0 149   0   0]\n",
            "1.0 1 [  0 101   0   0   0   0   0   0   0   0   0   0   0   0   0 133   0   0]\n",
            "1.0 0 [  0 117   0   0   0   0   0   0   0   0   0   0   0   0   0 117   0   0]\n",
            "1.0 0 [  0 133   0   0   0   0   0   0   0   0   0   0   0   0   0 101   0   0]\n",
            "1.0 0 [  0 149   0   0   0   0   0   0   0   0   0   0   0   0   0  85   0   0]\n",
            "1.0 0 [  0 165   0   0   0   0   0   0   0   0   0   0   0   0   0  69   0   0]\n",
            "1.0 0 [  0 181   0   0   0   0   0   0   0   0   0   0   0   0   0  53   0   0]\n",
            "1.0 0 [  0 197   0   0   0   0   0   0   0   0   0   0   0   0   0  37   0   0]\n",
            "1.0 0 [  0 213   0   0   0   0   0   0   0   0   0   0   0   0   0  21   0   0]\n",
            "[8.64319265e-01 9.46550906e-01 9.73417819e-01 9.84729409e-01\n",
            " 9.66516078e-01 7.99909770e-01 2.70931363e-01 3.53705883e-02\n",
            " 1.84284858e-02 1.98976211e-02 6.74195215e-02 2.41010904e-01\n",
            " 4.95603681e-01 8.09001476e-02 3.25481929e-02 1.23490803e-02\n",
            " 4.59416443e-03 5.41019905e-03 2.42098253e-02 8.37437510e-02\n",
            " 6.10352874e-01 7.56966293e-01 7.12101638e-01 7.42877185e-01\n",
            " 9.26549315e-01 9.96839523e-01 1.59099477e-06 6.56051498e-06\n",
            " 4.00143945e-05 6.96021016e-05 4.19490061e-05 1.52908960e-05\n",
            " 1.80075112e-05 1.98512480e-05 7.20673052e-05 1.75782276e-04\n",
            " 1.95221844e-04 8.74673016e-04 1.57708842e-02 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the prediction for each Brightness level\n",
        "#NB I should really separate them depending on whether the correct answer is 0 or 1, but I havent done it.\n",
        "#Also I should separate out according to retinal position..\n",
        "import matplotlib.pyplot as plt\n",
        "x_vals=np.zeros(len(X_test),int)\n",
        "for trial in range(len(X_test)):\n",
        "  x=np.where(X_test[trial]>0)[0][0]\n",
        "  x_vals[trial]=X_test[trial][x]\n",
        "print(x_vals)\n",
        "print(np.around(res_test,2))\n",
        "plt.scatter(x_vals,res_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "xVAHrZ8AI7nh",
        "outputId": "e2f3fb80-1d6d-4efa-97ad-8495141d7188"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 21  37  53  69  85 101 117 133 149 165 181 197 213  21  37  53  69  85\n",
            " 101 117 133 149 165 181 197 213  21  37  53  69  85 101 117 133 149 165\n",
            " 181 197 213  21  37  53  69  85 101 117 133 149 165 181 197 213]\n",
            "[0.86 0.95 0.97 0.98 0.97 0.8  0.27 0.04 0.02 0.02 0.07 0.24 0.5  0.08\n",
            " 0.03 0.01 0.   0.01 0.02 0.08 0.61 0.76 0.71 0.74 0.93 1.   0.   0.\n",
            " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.02 1.   1.   1.\n",
            " 1.   1.   1.   1.   1.   1.   1.   1.   1.   1.  ]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fd80e67b760>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comment on results:\n",
        "This is quite surprising. I was expecting the NN would not succeed in generalizing to non-trained brightnesses or \\retinal positions. In fact it does it pretty well. The trained brightnesses are 69 and 165. There's something I dont understand here."
      ],
      "metadata": {
        "id": "QaKQ2E2_mm0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train with CNN"
      ],
      "metadata": {
        "id": "stPC6iIy3Z5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=32, kernel_size=4, activation='relu', input_shape=(len(X_train[0]), 1)))\n",
        "model.add(MaxPooling1D(pool_size=4))\n",
        "#model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
        "#model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=40, batch_size=32, verbose=1)\n",
        "\n",
        "#loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "#print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "Rb8O58aCqQ4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94c18702-c8fe-446b-ec23-279d43743fb9"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "1/1 [==============================] - 1s 809ms/step - loss: 8.8570 - accuracy: 0.0000e+00\n",
            "Epoch 2/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.6070 - accuracy: 0.0000e+00\n",
            "Epoch 3/40\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.8526 - accuracy: 0.0000e+00\n",
            "Epoch 4/40\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.3750 - accuracy: 0.5000\n",
            "Epoch 5/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3002 - accuracy: 1.0000\n",
            "Epoch 6/40\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0215 - accuracy: 1.0000\n",
            "Epoch 7/40\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 8/40\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 9/40\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 10/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 11/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.6114e-04 - accuracy: 1.0000\n",
            "Epoch 12/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.1474e-04 - accuracy: 1.0000\n",
            "Epoch 13/40\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.0234e-04 - accuracy: 1.0000\n",
            "Epoch 14/40\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.1461e-04 - accuracy: 1.0000\n",
            "Epoch 15/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.4513e-04 - accuracy: 1.0000\n",
            "Epoch 16/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.8943e-04 - accuracy: 1.0000\n",
            "Epoch 17/40\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.5793e-04 - accuracy: 1.0000\n",
            "Epoch 18/40\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.3928e-04 - accuracy: 1.0000\n",
            "Epoch 19/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.2257e-04 - accuracy: 1.0000\n",
            "Epoch 20/40\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.0751e-04 - accuracy: 1.0000\n",
            "Epoch 21/40\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9389e-04 - accuracy: 1.0000\n",
            "Epoch 22/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.8150e-04 - accuracy: 1.0000\n",
            "Epoch 23/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.7021e-04 - accuracy: 1.0000\n",
            "Epoch 24/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5987e-04 - accuracy: 1.0000\n",
            "Epoch 25/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5018e-04 - accuracy: 1.0000\n",
            "Epoch 26/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.2116e-04 - accuracy: 1.0000\n",
            "Epoch 27/40\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.8983e-04 - accuracy: 1.0000\n",
            "Epoch 28/40\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5987e-04 - accuracy: 1.0000\n",
            "Epoch 29/40\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.3291e-04 - accuracy: 1.0000\n",
            "Epoch 30/40\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0955e-04 - accuracy: 1.0000\n",
            "Epoch 31/40\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.9850e-05 - accuracy: 1.0000\n",
            "Epoch 32/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.3528e-05 - accuracy: 1.0000\n",
            "Epoch 33/40\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.0176e-05 - accuracy: 1.0000\n",
            "Epoch 34/40\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.9345e-05 - accuracy: 1.0000\n",
            "Epoch 35/40\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.0603e-05 - accuracy: 1.0000\n",
            "Epoch 36/40\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.3564e-05 - accuracy: 1.0000\n",
            "Epoch 37/40\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.7900e-05 - accuracy: 1.0000\n",
            "Epoch 38/40\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.3335e-05 - accuracy: 1.0000\n",
            "Epoch 39/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.9649e-05 - accuracy: 1.0000\n",
            "Epoch 40/40\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.6662e-05 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd80e63d0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for trial in range(len(X_test)):\n",
        "  pred=model.predict(np.array([X_test[trial]]), verbose=0)\n",
        "  res_test[trial]=pred[0][0]\n",
        "  #print(np.around(pred[0][0],1),y_test[trial],X_test[trial])\n",
        "  print(np.around(res_test[trial],1),y_test[trial],X_test[trial])\n",
        "\n",
        "print(res_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqozJP4KUx8p",
        "outputId": "91f09f31-36a3-44f3-c598-95c241f26146"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0 1 [  0   0   0   0  21   0   0   0   0   0   0   0 213   0   0   0   0   0]\n",
            "1.0 1 [  0   0   0   0  37   0   0   0   0   0   0   0 197   0   0   0   0   0]\n",
            "1.0 1 [  0   0   0   0  53   0   0   0   0   0   0   0 181   0   0   0   0   0]\n",
            "1.0 1 [  0   0   0   0  69   0   0   0   0   0   0   0 165   0   0   0   0   0]\n",
            "1.0 1 [  0   0   0   0  85   0   0   0   0   0   0   0 149   0   0   0   0   0]\n",
            "1.0 1 [  0   0   0   0 101   0   0   0   0   0   0   0 133   0   0   0   0   0]\n",
            "1.0 0 [  0   0   0   0 117   0   0   0   0   0   0   0 117   0   0   0   0   0]\n",
            "0.4 0 [  0   0   0   0 133   0   0   0   0   0   0   0 101   0   0   0   0   0]\n",
            "0.0 0 [  0   0   0   0 149   0   0   0   0   0   0   0  85   0   0   0   0   0]\n",
            "0.0 0 [  0   0   0   0 165   0   0   0   0   0   0   0  69   0   0   0   0   0]\n",
            "0.0 0 [  0   0   0   0 181   0   0   0   0   0   0   0  53   0   0   0   0   0]\n",
            "0.0 0 [  0   0   0   0 197   0   0   0   0   0   0   0  37   0   0   0   0   0]\n",
            "0.0 0 [  0   0   0   0 213   0   0   0   0   0   0   0  21   0   0   0   0   0]\n",
            "1.0 1 [  0   0   0  21   0   0   0   0   0   0   0   0   0 213   0   0   0   0]\n",
            "1.0 1 [  0   0   0  37   0   0   0   0   0   0   0   0   0 197   0   0   0   0]\n",
            "1.0 1 [  0   0   0  53   0   0   0   0   0   0   0   0   0 181   0   0   0   0]\n",
            "1.0 1 [  0   0   0  69   0   0   0   0   0   0   0   0   0 165   0   0   0   0]\n",
            "1.0 1 [  0   0   0  85   0   0   0   0   0   0   0   0   0 149   0   0   0   0]\n",
            "1.0 1 [  0   0   0 101   0   0   0   0   0   0   0   0   0 133   0   0   0   0]\n",
            "1.0 0 [  0   0   0 117   0   0   0   0   0   0   0   0   0 117   0   0   0   0]\n",
            "1.0 0 [  0   0   0 133   0   0   0   0   0   0   0   0   0 101   0   0   0   0]\n",
            "0.5 0 [  0   0   0 149   0   0   0   0   0   0   0   0   0  85   0   0   0   0]\n",
            "0.0 0 [  0   0   0 165   0   0   0   0   0   0   0   0   0  69   0   0   0   0]\n",
            "0.0 0 [  0   0   0 181   0   0   0   0   0   0   0   0   0  53   0   0   0   0]\n",
            "0.0 0 [  0   0   0 197   0   0   0   0   0   0   0   0   0  37   0   0   0   0]\n",
            "0.0 0 [  0   0   0 213   0   0   0   0   0   0   0   0   0  21   0   0   0   0]\n",
            "1.0 1 [  0   0  21   0   0   0   0   0   0   0   0   0   0   0 213   0   0   0]\n",
            "1.0 1 [  0   0  37   0   0   0   0   0   0   0   0   0   0   0 197   0   0   0]\n",
            "1.0 1 [  0   0  53   0   0   0   0   0   0   0   0   0   0   0 181   0   0   0]\n",
            "1.0 1 [  0   0  69   0   0   0   0   0   0   0   0   0   0   0 165   0   0   0]\n",
            "1.0 1 [  0   0  85   0   0   0   0   0   0   0   0   0   0   0 149   0   0   0]\n",
            "1.0 1 [  0   0 101   0   0   0   0   0   0   0   0   0   0   0 133   0   0   0]\n",
            "1.0 0 [  0   0 117   0   0   0   0   0   0   0   0   0   0   0 117   0   0   0]\n",
            "1.0 0 [  0   0 133   0   0   0   0   0   0   0   0   0   0   0 101   0   0   0]\n",
            "1.0 0 [  0   0 149   0   0   0   0   0   0   0   0   0   0   0  85   0   0   0]\n",
            "0.9 0 [  0   0 165   0   0   0   0   0   0   0   0   0   0   0  69   0   0   0]\n",
            "0.6 0 [  0   0 181   0   0   0   0   0   0   0   0   0   0   0  53   0   0   0]\n",
            "0.3 0 [  0   0 197   0   0   0   0   0   0   0   0   0   0   0  37   0   0   0]\n",
            "0.1 0 [  0   0 213   0   0   0   0   0   0   0   0   0   0   0  21   0   0   0]\n",
            "0.4 1 [  0  21   0   0   0   0   0   0   0   0   0   0   0   0   0 213   0   0]\n",
            "0.3 1 [  0  37   0   0   0   0   0   0   0   0   0   0   0   0   0 197   0   0]\n",
            "0.3 1 [  0  53   0   0   0   0   0   0   0   0   0   0   0   0   0 181   0   0]\n",
            "0.2 1 [  0  69   0   0   0   0   0   0   0   0   0   0   0   0   0 165   0   0]\n",
            "0.2 1 [  0  85   0   0   0   0   0   0   0   0   0   0   0   0   0 149   0   0]\n",
            "0.1 1 [  0 101   0   0   0   0   0   0   0   0   0   0   0   0   0 133   0   0]\n",
            "0.1 0 [  0 117   0   0   0   0   0   0   0   0   0   0   0   0   0 117   0   0]\n",
            "0.1 0 [  0 133   0   0   0   0   0   0   0   0   0   0   0   0   0 101   0   0]\n",
            "0.1 0 [  0 149   0   0   0   0   0   0   0   0   0   0   0   0   0  85   0   0]\n",
            "0.0 0 [  0 165   0   0   0   0   0   0   0   0   0   0   0   0   0  69   0   0]\n",
            "0.0 0 [  0 181   0   0   0   0   0   0   0   0   0   0   0   0   0  53   0   0]\n",
            "0.0 0 [  0 197   0   0   0   0   0   0   0   0   0   0   0   0   0  37   0   0]\n",
            "0.0 0 [  0 213   0   0   0   0   0   0   0   0   0   0   0   0   0  21   0   0]\n",
            "[1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 9.99994278e-01 9.97328162e-01 4.43114161e-01\n",
            " 3.56746512e-03 2.84615908e-05 2.33303027e-07 6.34734709e-09\n",
            " 1.75401166e-10 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 9.99996603e-01 9.99663711e-01\n",
            " 9.74898875e-01 5.49853086e-01 4.05626372e-02 2.12597544e-03\n",
            " 2.03750955e-04 2.38742123e-05 9.99999344e-01 9.99999821e-01\n",
            " 9.99999881e-01 9.99999821e-01 9.99998569e-01 9.99980748e-01\n",
            " 9.99737084e-01 9.97198522e-01 9.77375805e-01 8.61456156e-01\n",
            " 5.83324373e-01 2.65279919e-01 1.19875364e-01 4.05179054e-01\n",
            " 3.32136244e-01 2.66364694e-01 2.09531218e-01 1.62143812e-01\n",
            " 1.23795502e-01 9.35045332e-02 7.00328797e-02 5.21144904e-02\n",
            " 3.85904610e-02 2.84703821e-02 2.09466759e-02 1.53795090e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the prediction for each Brightness level\n",
        "#NB I should really separate them depending on whether the correct answer is 0 or 1, but I havent done it.\n",
        "#Also I should separate out according to retinal position..\n",
        "import matplotlib.pyplot as plt\n",
        "x_vals=np.zeros(len(X_test),int)\n",
        "for trial in range(len(X_test)):\n",
        "  x=np.where(X_test[trial]>0)[0][0]\n",
        "  x_vals[trial]=X_test[trial][x]\n",
        "print(x_vals)\n",
        "print(np.around(res_test,2))\n",
        "plt.scatter(x_vals,res_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "0DKN_c14UuD1",
        "outputId": "9aaff701-f365-4aff-cebc-51bdc10620ba"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 21  37  53  69  85 101 117 133 149 165 181 197 213  21  37  53  69  85\n",
            " 101 117 133 149 165 181 197 213  21  37  53  69  85 101 117 133 149 165\n",
            " 181 197 213  21  37  53  69  85 101 117 133 149 165 181 197 213]\n",
            "[1.   1.   1.   1.   1.   1.   1.   0.44 0.   0.   0.   0.   0.   1.\n",
            " 1.   1.   1.   1.   1.   1.   0.97 0.55 0.04 0.   0.   0.   1.   1.\n",
            " 1.   1.   1.   1.   1.   1.   0.98 0.86 0.58 0.27 0.12 0.41 0.33 0.27\n",
            " 0.21 0.16 0.12 0.09 0.07 0.05 0.04 0.03 0.02 0.02]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fd811463d00>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test CNN \n"
      ],
      "metadata": {
        "id": "AfbygbGhGyHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comment on results:\n",
        "Very odd that the CNN does worse than the NN, with results for the higher brightnesses completely wrong."
      ],
      "metadata": {
        "id": "OyZvgc1qOK4j"
      }
    }
  ]
}